# Guide Producer/Consumer - CryptoViz Pipeline

Documentation complÃ¨te du dÃ©veloppement du pipeline Kafka â†’ Spark â†’ InfluxDB pour le projet CryptoViz.

---

## Table des matiÃ¨res
1. [Architecture finale](#architecture-finale)
2. [ProblÃ¨mes rencontrÃ©s et solutions](#problÃ¨mes-rencontrÃ©s-et-solutions)
3. [Configuration du Producer](#configuration-du-producer)
4. [Configuration du Consumer](#configuration-du-consumer)
5. [Commandes utiles](#commandes-utiles)
6. [Tests et validation](#tests-et-validation)

---

## Architecture finale

```
CoinGecko API â†’ Producer (Python/kafka-python) â†’ Kafka â†’ Consumer (Spark Streaming) â†’ InfluxDB â†’ Grafana/API
```

### Stack technique
- **Producer**: Python 3.11, kafka-python, requests
- **Message Broker**: Apache Kafka 7.4.0 (Confluent)
- **Consumer**: PySpark 3.5.0, Structured Streaming
- **Base de donnÃ©es**: InfluxDB 2.7 (time-series)
- **Visualisation**: Grafana 10.0.0
- **API/Web**: FastAPI + Next.js 14

### Cryptos surveillÃ©s
- bitcoin
- cardano
- ethereum
- polkadot
- solana

---

## ProblÃ¨mes rencontrÃ©s et solutions

### 1. âŒ Producer ne pouvait pas envoyer de messages

**SymptÃ´me:**
```
NoBrokersAvailable: NoBrokersAvailable
```

**Causes:**
1. Kafka n'Ã©tait pas correctement dÃ©marrÃ©
2. Mauvaise configuration des listeners Kafka
3. Type du paramÃ¨tre `acks` incorrect

**Solutions:**
```yaml
# docker-compose.yml - Configuration Kafka corrigÃ©e
environment:
  KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092  # âœ… Ã‰coute sur toutes les interfaces
  KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
```

```python
# crypto_producer.py - Configuration producer corrigÃ©e
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
    acks=1,  # âœ… Type int, pas string 'all'
    retries=3,
    linger_ms=0,
    batch_size=16384
)
```

**MÃ©thode de flush optimisÃ©e:**
```python
# âœ… Flush AVANT l'envoi pour garantir la livraison
producer.flush()
for crypto in CRYPTOS:
    data = fetch_crypto_data(crypto)
    future = producer.send(TOPIC, value=data)
    # Pas de flush ici, dÃ©jÃ  fait avant
```

---

### 2. âŒ Consumer n'Ã©crivait pas dans InfluxDB

**SymptÃ´me:**
```
Batch 123 processed: 5 rows
# Mais rien dans InfluxDB
```

**Causes:**
1. Ã‰criture asynchrone qui ne finalisait pas
2. Pas de gestion d'erreur visible
3. Mode foreachBatch mal configurÃ©

**Solutions:**

```python
# crypto_consumer_spark.py - Ã‰criture synchrone forcÃ©e
def write_batch_to_influx(batch_df, batch_id):
    rows = batch_df.collect()
    client = InfluxDBClient(url=INFLUX_URL, token=INFLUX_TOKEN, org=INFLUX_ORG)
    write_api = client.write_api(write_options=SYNCHRONOUS)  # âœ… Mode synchrone
    
    for row in rows:
        point = Point("crypto_price") \
            .tag("crypto", row.crypto) \
            .field("price_usd", float(row.price_usd)) \
            .field("price_eur", float(row.price_eur)) \
            .field("change_24h", float(row.change_24h)) \
            .field("market_cap", float(row.market_cap)) \
            .field("volume_24h", float(row.volume_24h)) \
            .time(datetime.utcnow(), WritePrecision.NS)
        
        write_api.write(bucket=INFLUX_BUCKET, org=INFLUX_ORG, record=point)
    
    write_api.close()
    client.close()
    print(f"âœ… Batch {batch_id} Ã©crit: {len(rows)} points dans InfluxDB")
```

**Diagnostics ajoutÃ©s:**
```python
# VÃ©rification en temps rÃ©el
print(f"ðŸ“Š Batch {batch_id} reÃ§u: {batch_df.count()} lignes")
batch_df.show(5, truncate=False)  # Affiche les donnÃ©es
```

---

### 3. âŒ Offsets Kafka ne progressaient pas

**SymptÃ´me:**
```bash
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 --topic crypto-prices --time -1
# Toujours crypto-prices:0:0
```

**Causes:**
1. Producer n'envoyait pas rÃ©ellement (flush manquant)
2. Configuration `acks` incorrecte
3. Kafka venait de redÃ©marrer

**Solutions:**
```python
# Pattern flush-first
producer.flush()  # âœ… Vide le buffer d'abord
for crypto in CRYPTOS:
    future = producer.send(TOPIC, value=data)
    try:
        record_metadata = future.get(timeout=10)
        print(f"âœ… {crypto}: partition {record_metadata.partition}, offset {record_metadata.offset}")
    except Exception as e:
        print(f"âŒ Erreur envoi {crypto}: {e}")
```

**Validation:**
```bash
# VÃ©rifier les offsets aprÃ¨s chaque poll
docker exec kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 --topic crypto-prices --time -1
# âœ… crypto-prices:0:1234 (nombre qui augmente)
```

---

### 4. âŒ IncompatibilitÃ© Java 17 avec Spark

**SymptÃ´me:**
```
java.lang.IllegalAccessError: class org.apache.spark...
```

**Cause:**
Spark 3.5 nÃ©cessite Java 17 (pas Java 8)

**Solution:**
```bash
# macOS
brew install openjdk@17
export JAVA_HOME=$(/usr/libexec/java_home -v 17)

# VÃ©rification
java -version
# openjdk version "17.0.x"
```

**Configuration Spark:**
```python
spark = SparkSession.builder \
    .appName("CryptoConsumer") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
    .getOrCreate()
```

---

### 5. âŒ DonnÃ©es visibles dans InfluxDB UI mais pas dans Grafana

**SymptÃ´me:**
- Data Explorer InfluxDB: âœ… DonnÃ©es prÃ©sentes
- Grafana: âŒ "No data"

**Causes:**
1. Mauvais filtre de temps (range trop court)
2. Regex variable mal configurÃ©
3. Pas d'aggregateWindow

**Solutions:**

**Query Flux corrigÃ©e:**
```flux
from(bucket: "crypto-data")
  |> range(start: -24h)  # âœ… Assez large pour voir des donnÃ©es
  |> filter(fn: (r) => r._measurement == "crypto_price")
  |> filter(fn: (r) => r._field == "price_usd")
  |> filter(fn: (r) => r.crypto =~ /^${crypto:regex}$/)  # âœ… Regex anchored
  |> aggregateWindow(every: 5m, fn: last, createEmpty: false)  # âœ… RÃ©duit les points
```

**Variable Grafana:**
```json
{
  "name": "crypto",
  "type": "query",
  "query": "import \"influxdata/influxdb/schema\"\nschema.tagValues(bucket: \"crypto-data\", tag: \"crypto\", predicate: (r) => r._measurement == \"crypto_price\", start: -90d)",
  "regex": "^(bitcoin|cardano|ethereum|polkadot|solana)$",  // âœ… Restreint aux 5 cryptos
  "allValue": "^(bitcoin|cardano|ethereum|polkadot|solana)$",  // âœ… All = regex complet
  "multi": true,
  "includeAll": true
}
```

---

## Configuration du Producer

### Fichier: `crypto_producer.py`

**ParamÃ¨tres clÃ©s:**
```python
CRYPTOS = ["bitcoin", "ethereum", "cardano", "solana", "polkadot"]
TOPIC = "crypto-prices"
KAFKA_BROKER = "localhost:9092"

# Configuration optimale
PRODUCER_CONFIG = {
    'bootstrap_servers': KAFKA_BROKER,
    'value_serializer': lambda v: json.dumps(v).encode('utf-8'),
    'acks': 1,  # Balance entre performance et fiabilitÃ©
    'retries': 3,
    'linger_ms': 0,  # Pas de batching, envoi immÃ©diat
    'batch_size': 16384,
    'compression_type': None
}
```

**Variables d'environnement (optionnelles):**
```bash
export SEND_EVERY_POLL=1          # 1=flush avant chaque poll, 0=flush aprÃ¨s
export POLL_INTERVAL_SEC=10       # Intervalle entre les polls API (secondes)
export PRODUCER_ACKS=1            # 0, 1, ou 'all'
export PRODUCER_LINGER_MS=0       # DÃ©lai d'attente avant envoi
export PRODUCER_BATCH_SIZE=16384  # Taille max du batch
export PRODUCER_COMPRESSION=none  # none, gzip, snappy, lz4
```

**Structure des messages Kafka:**
```json
{
  "crypto": "bitcoin",
  "price_usd": 109724.5,
  "price_eur": 98432.1,
  "change_24h": 2.34,
  "market_cap": 2186543028396,
  "volume_24h": 45123456789,
  "timestamp": "2025-10-31T10:30:45.123456"
}
```

**Lancement:**
```bash
cd crypto-monitoring
source .venv/bin/activate

# MÃ©thode 1: Script
./run_producer.sh

# MÃ©thode 2: Manuel
export SEND_EVERY_POLL=1
export POLL_INTERVAL_SEC=10
python3 crypto_producer.py
```

---

## Configuration du Consumer

### Fichier: `crypto_consumer_spark.py`

**Configuration Spark:**
```python
spark = SparkSession.builder \
    .appName("CryptoConsumer") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
    .getOrCreate()

# Kafka source
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "crypto-prices") \
    .option("startingOffsets", "earliest") \
    .load()
```

**SchÃ©ma de parsing:**
```python
schema = StructType([
    StructField("crypto", StringType(), True),
    StructField("price_usd", DoubleType(), True),
    StructField("price_eur", DoubleType(), True),
    StructField("change_24h", DoubleType(), True),
    StructField("market_cap", DoubleType(), True),
    StructField("volume_24h", DoubleType(), True),
    StructField("timestamp", StringType(), True)
])
```

**Ã‰criture InfluxDB (SYNCHRONE):**
```python
from influxdb_client import InfluxDBClient, Point, WritePrecision
from influxdb_client.client.write_api import SYNCHRONOUS

def write_batch_to_influx(batch_df, batch_id):
    rows = batch_df.collect()
    if len(rows) == 0:
        return
    
    client = InfluxDBClient(url=INFLUX_URL, token=INFLUX_TOKEN, org=INFLUX_ORG)
    write_api = client.write_api(write_options=SYNCHRONOUS)
    
    for row in rows:
        point = Point("crypto_price") \
            .tag("crypto", row.crypto) \
            .field("price_usd", float(row.price_usd)) \
            .field("price_eur", float(row.price_eur)) \
            .field("change_24h", float(row.change_24h)) \
            .field("market_cap", float(row.market_cap)) \
            .field("volume_24h", float(row.volume_24h)) \
            .time(datetime.utcnow(), WritePrecision.NS)
        
        write_api.write(bucket=INFLUX_BUCKET, org=INFLUX_ORG, record=point)
    
    write_api.close()
    client.close()
```

**Lancement:**
```bash
cd crypto-monitoring
source .venv/bin/activate

# MÃ©thode 1: Script
./run_consumer.sh

# MÃ©thode 2: Manuel avec Java 17
export JAVA_HOME=$(/usr/libexec/java_home -v 17)
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  crypto_consumer_spark.py
```

---

## Commandes utiles

### Docker & Kafka

```bash
# DÃ©marrer les services
cd crypto-monitoring
docker compose up -d

# VÃ©rifier les services
docker ps
docker compose logs -f kafka

# VÃ©rifier que Kafka est prÃªt
docker exec -it kafka kafka-broker-api-versions --bootstrap-server localhost:9092

# Lister les topics
docker exec -it kafka kafka-topics --bootstrap-server localhost:9092 --list

# Voir les offsets (progression)
docker exec -it kafka kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic crypto-prices \
  --time -1

# Consommer des messages (debug)
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic crypto-prices \
  --from-beginning \
  --max-messages 5

# RÃ©initialiser les offsets (si besoin)
docker exec -it kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --group crypto-consumer-group \
  --reset-offsets \
  --to-earliest \
  --topic crypto-prices \
  --execute
```

### InfluxDB

```bash
# AccÃ¨s UI
open http://localhost:8086
# User: admin / Pass: adminpassword
# Org: crypto-org
# Bucket: crypto-data

# CLI: Lister les measurements
docker exec -it influxdb influx query \
  'from(bucket:"crypto-data") |> range(start:-1h) |> group(columns:["_measurement"]) |> distinct(column:"_measurement")'

# Compter les points par crypto
docker exec -it influxdb influx query \
  'from(bucket:"crypto-data") |> range(start:-24h) |> filter(fn:(r)=>r._measurement=="crypto_price") |> group(columns:["crypto"]) |> count()'

# Supprimer les sÃ©ries de test
python3 influx_delete_test_series.py
```

### Grafana

```bash
# AccÃ¨s UI
open http://localhost:3000
# User: admin / Pass: admin

# RedÃ©marrer pour recharger les dashboards
docker compose restart grafana

# VÃ©rifier les dashboards provisionnÃ©s
docker exec -it grafana ls /var/lib/grafana/dashboards
```

### Python environments

```bash
# CrÃ©er le venv (premiÃ¨re fois)
cd crypto-monitoring
python3 -m venv .venv

# Activer
source .venv/bin/activate

# Installer les dÃ©pendances
pip install -r requirements.txt

# VÃ©rifier les packages
pip list | grep -E 'kafka|spark|influx'
```

---

## Tests et validation

### Test end-to-end complet

```bash
# 1. Services Docker
cd crypto-monitoring
docker compose up -d
sleep 30  # Attendre Kafka

# 2. VÃ©rifier Kafka prÃªt
docker exec -it kafka kafka-broker-api-versions --bootstrap-server localhost:9092

# 3. Lancer le producer
source .venv/bin/activate
export SEND_EVERY_POLL=1
export POLL_INTERVAL_SEC=10
python3 crypto_producer.py &
PRODUCER_PID=$!

# 4. VÃ©rifier les messages Kafka
sleep 15
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic crypto-prices \
  --from-beginning \
  --max-messages 2

# 5. Lancer le consumer
export JAVA_HOME=$(/usr/libexec/java_home -v 17)
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 crypto_consumer_spark.py &
CONSUMER_PID=$!

# 6. Attendre quelques batchs
sleep 60

# 7. VÃ©rifier InfluxDB
open http://localhost:8086
# Data Explorer:
# - FROM crypto-data
# - FILTER _measurement = crypto_price
# - FILTER crypto = bitcoin
# - FIELD price_usd
# - RANGE last 1h
# âœ… Doit afficher des points

# 8. VÃ©rifier Grafana
open http://localhost:3000/d/crypto-core/crypto-core
# âœ… Les graphiques doivent afficher des donnÃ©es

# 9. Nettoyer
kill $PRODUCER_PID $CONSUMER_PID
```

### Validation des donnÃ©es

**VÃ©rifier la structure dans InfluxDB:**
```flux
from(bucket: "crypto-data")
  |> range(start: -1h)
  |> filter(fn: (r) => r._measurement == "crypto_price")
  |> filter(fn: (r) => r.crypto == "bitcoin")
  |> limit(n: 5)
```

**Attendu:**
```
_time                    _measurement   crypto   _field       _value
2025-10-31T10:30:45Z    crypto_price   bitcoin  price_usd    109724.5
2025-10-31T10:30:45Z    crypto_price   bitcoin  price_eur    98432.1
2025-10-31T10:30:45Z    crypto_price   bitcoin  change_24h   2.34
2025-10-31T10:30:45Z    crypto_price   bitcoin  market_cap   2186543028396
2025-10-31T10:30:45Z    crypto_price   bitcoin  volume_24h   45123456789
```

---

## Troubleshooting rapide

| ProblÃ¨me | VÃ©rification | Solution |
|----------|--------------|----------|
| Producer: NoBrokersAvailable | `docker ps \| grep kafka` | `docker compose restart kafka`, attendre 30s |
| Consumer: No data | Logs Spark | VÃ©rifier `write_batch_to_influx` appelÃ©, mode SYNCHRONOUS |
| Grafana: No data | InfluxDB UI | VÃ©rifier range, regex variable, aggregateWindow |
| Offsets bloquÃ©s Ã  0 | GetOffsetShell | Producer flush-first pattern, vÃ©rifier acks |
| Java IllegalAccessError | `java -version` | Installer Java 17, `export JAVA_HOME` |

---

## RÃ©sumÃ© des leÃ§ons apprises

1. **Kafka Producer:**
   - âœ… `acks=1` (int) pas `'all'` (string)
   - âœ… Flush AVANT l'envoi pour garantir livraison
   - âœ… VÃ©rifier `future.get()` pour confirmer envoi

2. **Spark Consumer:**
   - âœ… Mode SYNCHRONOUS obligatoire pour InfluxDB
   - âœ… Java 17 requis pour Spark 3.5
   - âœ… foreachBatch avec diagnostics print()

3. **InfluxDB:**
   - âœ… WritePrecision.NS pour timestamp prÃ©cis
   - âœ… Tag = crypto, Fields = metrics numÃ©riques
   - âœ… Data Explorer pour debug rapide

4. **Grafana:**
   - âœ… Flux avec aggregateWindow pour performance
   - âœ… Regex anchored `^...$` pour variables
   - âœ… fill(usePrevious) pour Ã©viter les gaps

5. **Debugging:**
   - âœ… Toujours vÃ©rifier bout Ã  bout: Kafka offsets â†’ InfluxDB UI â†’ Grafana
   - âœ… Logs verbose essentiels (print batch_id, count)
   - âœ… Scripts de run avec env vars pour reproductibilitÃ©

---

## Fichiers importants

```
crypto-monitoring/
â”œâ”€â”€ crypto_producer.py              # Producer Kafka
â”œâ”€â”€ crypto_consumer_spark.py        # Consumer Spark â†’ InfluxDB
â”œâ”€â”€ run_producer.sh                 # Script lancement producer
â”œâ”€â”€ run_consumer.sh                 # Script lancement consumer
â”œâ”€â”€ docker-compose.yml              # Kafka/Zookeeper/InfluxDB/Grafana
â”œâ”€â”€ requirements.txt                # DÃ©pendances Python
â”œâ”€â”€ influx_delete_test_series.py   # Nettoyage sÃ©ries test
â””â”€â”€ grafana/
    â”œâ”€â”€ provisioning/
    â”‚   â”œâ”€â”€ datasources/influxdb.yml
    â”‚   â””â”€â”€ dashboards/dashboard.yml
    â””â”€â”€ dashboards/
        â”œâ”€â”€ crypto_core.json
        â””â”€â”€ comparisons.json
```

---

**DerniÃ¨re mise Ã  jour:** 31 octobre 2025  
**Auteur:** DÃ©veloppement itÃ©ratif avec GitHub Copilot  
**Stack:** Kafka 7.4 + Spark 3.5 + InfluxDB 2.7 + Grafana 10.0
