#!/usr/bin/env python3
"""
Spark Structured Streaming Consumer - Anomaly Detection
DÃ©tecte les anomalies dans les donnÃ©es crypto en temps rÃ©el:
- Pics de volume anormaux (>3Ïƒ de la moyenne)
- Variations de prix extrÃªmes (>5% en <1min)
- Ã‰carts entre sources (CoinGecko vs CoinMarketCap)
- Stocke les alertes dans InfluxDB (measurement: crypto_anomalies)
"""
import os
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
from influxdb_client import InfluxDBClient, Point
from influxdb_client.client.write_api import SYNCHRONOUS
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Configuration InfluxDB
INFLUX_URL = os.getenv('INFLUX_URL', 'http://localhost:8086')
INFLUX_TOKEN = os.getenv('INFLUX_TOKEN')
INFLUX_ORG = os.getenv('INFLUX_ORG', 'crypto-org')
INFLUX_BUCKET = os.getenv('INFLUX_BUCKET', 'crypto-data')

# Configuration Kafka
KAFKA_BROKER = os.getenv('KAFKA_BROKER', 'localhost:9092')
TOPIC = 'crypto-prices'

# Seuils d'anomalies
VOLUME_SIGMA_THRESHOLD = 3.0  # Volume >3Ïƒ = anomalie
PRICE_CHANGE_THRESHOLD = 5.0  # Variation >5% = anomalie

# Client InfluxDB global
influx_client = None
write_api = None

# Cache pour stocker les derniÃ¨res valeurs (dÃ©tection de variations rapides)
price_cache = {}  # {crypto_id: {'price': float, 'timestamp': datetime}}


def get_influx_client():
    """Initialise et retourne le client InfluxDB"""
    global influx_client, write_api

    if influx_client is None:
        influx_client = InfluxDBClient(
            url=INFLUX_URL,
            token=INFLUX_TOKEN,
            org=INFLUX_ORG
        )
        write_api = influx_client.write_api(write_options=SYNCHRONOUS)
        print(f"âœ… InfluxDB connectÃ©: {INFLUX_URL}")

    return influx_client, write_api


def detect_anomalies(batch_df, batch_id):
    """
    DÃ©tecte les anomalies dans un batch de donnÃ©es.

    Anomalies dÃ©tectÃ©es:
    1. Volume anormal (>3Ïƒ de la moyenne historique)
    2. Variation de prix rapide (>5% en moins d'1 minute)
    3. Prix trÃ¨s diffÃ©rent entre sources

    Args:
        batch_df: DataFrame Spark
        batch_id: ID du batch
    """
    if batch_df.isEmpty():
        print(f"â„¹ï¸  Batch {batch_id}: Aucune donnÃ©e Ã  analyser")
        return

    print(f"\nðŸ” Batch {batch_id}: DÃ©tection d'anomalies...")

    # Convertir en Pandas pour analyse
    pdf = batch_df.select(
        "crypto_id",
        "symbol",
        "name",
        "source",
        "price_usd",
        "volume_24h",
        "change_1h",
        "timestamp"
    ).toPandas()

    if pdf.empty:
        return

    anomalies = []

    # Analyser par crypto
    for crypto_id in pdf['crypto_id'].unique():
        crypto_data = pdf[pdf['crypto_id'] == crypto_id]

        if len(crypto_data) == 0:
            continue

        symbol = crypto_data['symbol'].iloc[0]
        name = crypto_data['name'].iloc[0]

        # 1. DÃ©tection de volume anormal
        volume_mean = crypto_data['volume_24h'].mean()
        volume_std = crypto_data['volume_24h'].std()

        if volume_std and volume_std > 0:
            for _, row in crypto_data.iterrows():
                volume_z_score = (row['volume_24h'] - volume_mean) / volume_std

                if abs(volume_z_score) > VOLUME_SIGMA_THRESHOLD:
                    anomalies.append({
                        'crypto_id': crypto_id,
                        'symbol': symbol,
                        'name': name,
                        'anomaly_type': 'volume_spike',
                        'severity': 'high' if abs(volume_z_score) > 4 else 'medium',
                        'value': row['volume_24h'],
                        'expected': volume_mean,
                        'z_score': volume_z_score,
                        'message': f"Volume anormal: {row['volume_24h']:,.0f} (moyenne: {volume_mean:,.0f}, z-score: {volume_z_score:.2f})",
                        'source': row['source'],
                        'timestamp': datetime.utcnow().isoformat()
                    })

        # 2. DÃ©tection de variation rapide de prix
        current_price = crypto_data['price_usd'].iloc[-1]

        if crypto_id in price_cache:
            last_price = price_cache[crypto_id]['price']
            price_change_pct = abs((current_price - last_price) / last_price * 100)

            if price_change_pct > PRICE_CHANGE_THRESHOLD:
                anomalies.append({
                    'crypto_id': crypto_id,
                    'symbol': symbol,
                    'name': name,
                    'anomaly_type': 'price_spike',
                    'severity': 'critical' if price_change_pct > 10 else 'high',
                    'value': current_price,
                    'expected': last_price,
                    'change_pct': price_change_pct,
                    'message': f"Variation rapide: {price_change_pct:.2f}% (${last_price:.2f} â†’ ${current_price:.2f})",
                    'source': crypto_data['source'].iloc[-1],
                    'timestamp': datetime.utcnow().isoformat()
                })

        # Mettre Ã  jour le cache
        price_cache[crypto_id] = {
            'price': current_price,
            'timestamp': datetime.utcnow()
        }

        # 3. DÃ©tection d'Ã©cart entre sources
        sources = crypto_data['source'].unique()
        if len(sources) > 1:
            prices_by_source = {}
            for source in sources:
                source_data = crypto_data[crypto_data['source'] == source]
                if not source_data.empty:
                    prices_by_source[source] = source_data['price_usd'].mean()

            if len(prices_by_source) >= 2:
                price_values = list(prices_by_source.values())
                price_diff_pct = abs((max(price_values) - min(price_values)) / min(price_values) * 100)

                if price_diff_pct > 1.0:  # Ã‰cart >1% entre sources
                    sources_str = ', '.join([f"{s}: ${p:.2f}" for s, p in prices_by_source.items()])
                    anomalies.append({
                        'crypto_id': crypto_id,
                        'symbol': symbol,
                        'name': name,
                        'anomaly_type': 'source_divergence',
                        'severity': 'medium' if price_diff_pct < 3 else 'high',
                        'value': max(price_values),
                        'expected': min(price_values),
                        'divergence_pct': price_diff_pct,
                        'message': f"Ã‰cart entre sources: {price_diff_pct:.2f}% ({sources_str})",
                        'source': 'multi-source',
                        'timestamp': datetime.utcnow().isoformat()
                    })

    # Ã‰crire les anomalies dÃ©tectÃ©es
    if anomalies:
        print(f"ðŸš¨ Batch {batch_id}: {len(anomalies)} anomalies dÃ©tectÃ©es!")
        write_anomalies_to_influx(anomalies, batch_id)
    else:
        print(f"âœ… Batch {batch_id}: Aucune anomalie dÃ©tectÃ©e")


def write_anomalies_to_influx(anomalies_list, batch_id):
    """
    Ã‰crit les anomalies dÃ©tectÃ©es dans InfluxDB.

    Args:
        anomalies_list: Liste de dictionnaires d'anomalies
        batch_id: ID du batch
    """
    _, write_api = get_influx_client()

    points = []
    for anomaly in anomalies_list:
        try:
            # CrÃ©er un point InfluxDB
            point = (
                Point("crypto_anomalies")
                .tag("crypto_id", anomaly['crypto_id'])
                .tag("symbol", anomaly['symbol'])
                .tag("anomaly_type", anomaly['anomaly_type'])
                .tag("severity", anomaly['severity'])
                .tag("source", anomaly['source'])
                .field("value", float(anomaly['value']))
                .field("expected", float(anomaly['expected']))
                .field("message", anomaly['message'])
            )

            # Ajouter les champs spÃ©cifiques selon le type
            if 'z_score' in anomaly:
                point = point.field("z_score", float(anomaly['z_score']))
            if 'change_pct' in anomaly:
                point = point.field("change_pct", float(anomaly['change_pct']))
            if 'divergence_pct' in anomaly:
                point = point.field("divergence_pct", float(anomaly['divergence_pct']))

            point = point.time(datetime.utcnow())
            points.append(point)

            # Afficher l'anomalie
            print(f"  ðŸš¨ [{anomaly['severity'].upper()}] {anomaly['symbol']}: {anomaly['message']}")

        except Exception as e:
            print(f"âš ï¸  Erreur crÃ©ation point anomalie pour {anomaly.get('crypto_id')}: {e}")
            continue

    # Ã‰crire en batch
    if points:
        try:
            write_api.write(bucket=INFLUX_BUCKET, record=points)
            print(f"âœ… Batch {batch_id}: {len(points)} anomalies Ã©crites dans InfluxDB (measurement: crypto_anomalies)")
        except Exception as e:
            print(f"âŒ Batch {batch_id}: Erreur Ã©criture anomalies: {e}")


def main():
    """Lance le consumer d'anomalies"""

    print("=" * 80)
    print("ðŸ” SPARK ANOMALY DETECTION CONSUMER - Real-time Crypto Anomaly Detection")
    print("=" * 80)
    print(f"ðŸ“¡ Kafka Broker: {KAFKA_BROKER}")
    print(f"ðŸ“Š InfluxDB: {INFLUX_URL}")
    print(f"ðŸª£ Bucket: {INFLUX_BUCKET}")
    print()
    print("ðŸš¨ Anomalies dÃ©tectÃ©es:")
    print(f"  â€¢ Volume anormal (>Â±{VOLUME_SIGMA_THRESHOLD}Ïƒ)")
    print(f"  â€¢ Variation de prix rapide (>Â±{PRICE_CHANGE_THRESHOLD}%)")
    print(f"  â€¢ Divergence entre sources (>1%)")
    print("=" * 80)
    print()

    # CrÃ©er la session Spark
    spark = SparkSession.builder \
        .appName("CryptoAnomalyDetectionConsumer") \
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1") \
        .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", "true") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")

    # SchÃ©ma des messages
    price_schema = StructType([
        StructField("crypto_id", StringType(), True),
        StructField("symbol", StringType(), True),
        StructField("name", StringType(), True),
        StructField("source", StringType(), True),
        StructField("price_usd", DoubleType(), True),
        StructField("volume_24h", DoubleType(), True),
        StructField("change_1h", DoubleType(), True),
        StructField("timestamp", StringType(), True),
    ])

    # Lire depuis Kafka
    df = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BROKER) \
        .option("subscribe", TOPIC) \
        .option("startingOffsets", "latest") \
        .option("failOnDataLoss", "false") \
        .load()

    # Parser le JSON
    crypto_df = df.select(
        from_json(col("value").cast("string"), price_schema).alias("data")
    ).select("data.*")

    print("ðŸš€ Consumer dÃ©marrÃ©, surveillance active...")
    print("   (Ctrl+C pour arrÃªter)")
    print()

    # DÃ©tecter les anomalies
    query = crypto_df \
        .writeStream \
        .foreachBatch(detect_anomalies) \
        .outputMode("append") \
        .option("checkpointLocation", "/tmp/spark-checkpoint-anomalies") \
        .trigger(processingTime="30 seconds") \
        .start()

    # Attendre l'arrÃªt
    try:
        query.awaitTermination()
    except KeyboardInterrupt:
        print("\n\nðŸ›‘ ArrÃªt du consumer d'anomalies...")
        query.stop()
        if influx_client:
            influx_client.close()
        spark.stop()
        print("ðŸ‘‹ Consumer d'anomalies arrÃªtÃ©")


if __name__ == "__main__":
    main()
