"""Classe de base pour tous les agents de scraping"""
import json
import time
import os
from abc import ABC, abstractmethod
from typing import List, Dict, Optional
from pathlib import Path
from kafka import KafkaProducer
from .config import KAFKA_BROKER, PRODUCER_CONFIG

# Import Avro pour validation
try:
    import avro.schema
    import avro.io
    AVRO_AVAILABLE = True
except ImportError:
    AVRO_AVAILABLE = False
    print("‚ö†Ô∏è  avro-python3 non install√© - Validation d√©sactiv√©e")

class BaseAgent(ABC):
    """
    Classe abstraite pour les agents de scraping.
    Tous les agents doivent h√©riter de cette classe et impl√©menter run().
    """
    
    def __init__(self, name: str, topic: str, poll_interval: int = 300, schema_file: Optional[str] = None):
        """
        Args:
            name: Nom de l'agent (ex: "FearGreedAgent")
            topic: Topic Kafka o√π envoyer les donn√©es
            poll_interval: Intervalle en secondes entre chaque collecte
            schema_file: Chemin vers le sch√©ma Avro (.avsc) pour validation (optionnel)
        """
        self.name = name
        self.topic = topic
        self.poll_interval = poll_interval
        self.producer = None
        self.schema = None
        
        # Charger le sch√©ma Avro si fourni
        if schema_file and AVRO_AVAILABLE:
            self._load_schema(schema_file)
    
    def _load_schema(self, schema_file: str):
        """Charge un sch√©ma Avro depuis un fichier .avsc"""
        try:
            schema_path = Path(__file__).parent.parent / schema_file
            with open(schema_path, 'r') as f:
                schema_dict = json.load(f)
                self.schema = avro.schema.parse(json.dumps(schema_dict))
                print(f"‚úÖ [{self.name}] Sch√©ma Avro charg√©: {schema_file}")
        except Exception as e:
            print(f"‚ö†Ô∏è  [{self.name}] Impossible de charger le sch√©ma {schema_file}: {e}")
            self.schema = None
    
    def validate_data(self, data: dict) -> tuple[bool, Optional[str]]:
        """
        Valide un dictionnaire contre le sch√©ma Avro.
        
        Args:
            data: Dictionnaire √† valider
            
        Returns:
            (is_valid, error_message): Tuple (bool, str ou None)
        """
        if not self.schema or not AVRO_AVAILABLE:
            return True, None  # Pas de validation si pas de sch√©ma
        
        try:
            # V√©rifier que toutes les cl√©s obligatoires sont pr√©sentes
            # et que les types correspondent
            import io
            writer = avro.io.DatumWriter(self.schema)
            bytes_writer = io.BytesIO()
            encoder = avro.io.BinaryEncoder(bytes_writer)
            writer.write(data, encoder)
            return True, None
        except Exception as e:
            return False, str(e)
        
    def connect_kafka(self):
        """Cr√©e la connexion au producer Kafka"""
        try:
            self.producer = KafkaProducer(
                bootstrap_servers=KAFKA_BROKER,
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                **PRODUCER_CONFIG
            )
            print(f"‚úÖ [{self.name}] Connect√© √† Kafka ({KAFKA_BROKER})")
        except Exception as e:
            print(f"‚ùå [{self.name}] Erreur Kafka: {e}")
            raise
    
    def send_to_kafka(self, data: dict):
        """
        Envoie des donn√©es √† Kafka
        
        Args:
            data: Dictionnaire Python (sera converti en JSON)
        """
        if not self.producer:
            raise RuntimeError("Producer Kafka non initialis√©. Appelez connect_kafka() d'abord.")
        
        try:
            future = self.producer.send(self.topic, value=data)
            future.get(timeout=10)  # Bloque jusqu'√† confirmation
            print(f"üì§ [{self.name}] Envoy√© vers {self.topic}: {data}")
        except Exception as e:
            print(f"‚ùå [{self.name}] Erreur d'envoi: {e}")
            raise
    
    def send_batch_to_kafka(self, data_list: List[dict], debug: bool = False, validate: bool = True) -> Dict:
        """
        Envoie plusieurs messages en batch (async) pour optimiser les performances.
        
        Avantages:
        - Validation Avro avant envoi (optionnel)
        - Envoi asynchrone (pas d'attente entre chaque message)
        - V√©rification des erreurs √† la fin
        - Kafka batching automatique c√¥t√© producer
        
        Args:
            data_list: Liste de dictionnaires √† envoyer
            debug: Afficher les stats de performance (default: False)
            validate: Valider avec sch√©ma Avro avant envoi (default: True)
        
        Returns:
            dict: Stats {'success': int, 'errors': int, 'validation_errors': int, 'duration_ms': float}
        """
        if not self.producer:
            raise RuntimeError("Producer Kafka non initialis√©. Appelez connect_kafka() d'abord.")
        
        start_time = time.time()
        futures = []
        errors = []
        validation_errors = 0
        
        # Phase 1: Validation + Envoi asynchrone
        for data in data_list:
            # Validation Avro (si activ√©e et sch√©ma disponible)
            if validate and self.schema:
                is_valid, error_msg = self.validate_data(data)
                if not is_valid:
                    crypto_id = data.get('crypto_id', 'unknown')
                    print(f"‚ö†Ô∏è  [{self.name}] Validation √©chou√©e pour {crypto_id}: {error_msg}")
                    validation_errors += 1
                    continue  # Skip ce message invalide
            
            # Envoi asynchrone
            try:
                future = self.producer.send(self.topic, value=data)
                futures.append((future, data))
            except Exception as e:
                errors.append((data, str(e)))
        
        # Phase 2: V√©rifier les r√©sultats de chaque envoi
        for future, data in futures:
            try:
                # Attendre la confirmation (avec timeout)
                future.get(timeout=10)
            except Exception as e:
                errors.append((data, str(e)))
        
        duration_ms = (time.time() - start_time) * 1000
        success_count = len(data_list) - len(errors) - validation_errors
        
        # Stats de debugging (optionnel)
        if debug:
            print(f"üìä [{self.name}] Batch envoy√©: {success_count}/{len(data_list)} succ√®s en {duration_ms:.1f}ms")
            if validation_errors > 0:
                print(f"‚ö†Ô∏è  [{self.name}] {validation_errors} erreurs de validation")
            if errors:
                print(f"‚ö†Ô∏è  [{self.name}] {len(errors)} erreurs d'envoi")
        
        # Logger les erreurs d'envoi (toujours, m√™me sans debug)
        for failed_data, error in errors:
            crypto_id = failed_data.get('crypto_id', 'unknown')
            print(f"‚ùå [{self.name}] √âchec envoi {crypto_id}: {error}")
        
        return {
            'success': success_count,
            'errors': len(errors),
            'validation_errors': validation_errors,
            'duration_ms': round(duration_ms, 2)
        }
    
    @abstractmethod
    def fetch_data(self):
        """
        M√©thode abstraite : chaque agent doit impl√©menter sa logique de collecte.
        
        Returns:
            dict: Donn√©es collect√©es
        """
        pass
    
    def run(self):
        """
        Boucle principale de l'agent.
        Collecte les donn√©es et les envoie √† Kafka en continu.
        """
        self.connect_kafka()
        
        print(f" [{self.name}] D√©marrage (intervalle: {self.poll_interval}s)")
        
        try:
            while True:
                try:
                    # Collecter les donn√©es (impl√©ment√© par chaque agent)
                    data = self.fetch_data()
                    
                    # Envoyer √† Kafka
                    if data:
                        self.send_to_kafka(data)
                    
                    # Attendre avant la prochaine collecte
                    time.sleep(self.poll_interval)
                    
                except KeyboardInterrupt:
                    print(f"\n‚èπÔ∏è  [{self.name}] Arr√™t demand√©")
                    break
                except Exception as e:
                    print(f"‚ö†Ô∏è  [{self.name}] Erreur: {e}")
                    time.sleep(10)  # Attendre 10s avant de r√©essayer
                    
        finally:
            if self.producer:
                self.producer.flush()
                self.producer.close()
                print(f" [{self.name}] D√©connect√© de Kafka")